{"_path":"/essay/25.pytorch-神经网络基础","_dir":"essay","_draft":false,"_partial":false,"_locale":"","title":"pytorch-神经网络基础","description":"碎碎念：知识还没有完全还给老师 😄","subtitle":"pytorch neural network","index":25,"date":"2023-03-25 00:00:01","lunar_date":"闰二初四","year":"2023","month":"03","month_en":"Mar","day":"25","tag":"技术","tag_en":"TECH","cover":"/img/rabbit/025.jpg","categories":"python","mathjax":true,"excerpt":{"type":"root","children":[{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"碎碎念：知识还没有完全还给老师 😄"}]}]},"body":{"type":"root","children":[{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"碎碎念：知识还没有完全还给老师 😄"}]},{"type":"element","tag":"h1","props":{"id":"张量tensor"},"children":[{"type":"text","value":"张量(Tensor)"}]},{"type":"element","tag":"h2","props":{"id":"简单理解张量"},"children":[{"type":"text","value":"简单理解张量"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"张量-多维数组-维度则为数组嵌套的层数\n0维张量-标量"}]},{"type":"element","tag":"pre","props":{"className":["language-py"],"code":"[1]\n","language":"py","meta":""},"children":[{"type":"element","tag":"code","props":{"__ignoreMap":""},"children":[{"type":"text","value":"[1]\n"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"1维张量-向量"}]},{"type":"element","tag":"pre","props":{"className":["language-py"],"code":"[1,2,3]\n","language":"py","meta":""},"children":[{"type":"element","tag":"code","props":{"__ignoreMap":""},"children":[{"type":"text","value":"[1,2,3]\n"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"2维张量-矩阵(3x3)"}]},{"type":"element","tag":"pre","props":{"className":["language-py"],"code":"[\n    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n]\n","language":"py","meta":""},"children":[{"type":"element","tag":"code","props":{"__ignoreMap":""},"children":[{"type":"text","value":"[\n    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n]\n"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"3维张量(2x2x3)"}]},{"type":"element","tag":"pre","props":{"className":["language-py"],"code":"[\n    [\n        [1,2,3],\n        [4,5,6]\n    ],\n    [\n        [7,8,9],\n        [10,11,12]\n    ]\n]\n","language":"py","meta":""},"children":[{"type":"element","tag":"code","props":{"__ignoreMap":""},"children":[{"type":"text","value":"[\n    [\n        [1,2,3],\n        [4,5,6]\n    ],\n    [\n        [7,8,9],\n        [10,11,12]\n    ]\n]\n"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"更直观地类比:"}]},{"type":"element","tag":"pre","props":{"className":["language-py"],"code":"0维张量-点\n1维张量-线(长)\n2维张量-面(长、宽)\n3维张量-体(长、宽、高)\n4维张量-时空(再多一个时间维度)\n","language":"py","meta":""},"children":[{"type":"element","tag":"code","props":{"__ignoreMap":""},"children":[{"type":"text","value":"0维张量-点\n1维张量-线(长)\n2维张量-面(长、宽)\n3维张量-体(长、宽、高)\n4维张量-时空(再多一个时间维度)\n"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"维度可以说是描述事物的参数个数，例如要描述食物的色、香、味，则需要3个参数，即使用3维张量。"}]},{"type":"element","tag":"h2","props":{"id":"常用-api"},"children":[{"type":"text","value":"常用 API"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"a","props":{"href":"https://blog.csdn.net/qq_49134563/article/details/108200828","rel":["nofollow"]},"children":[{"type":"text","value":"https://blog.csdn.net/qq_49134563/article/details/108200828"}]}]},{"type":"element","tag":"h1","props":{"id":"微分与梯度"},"children":[{"type":"text","value":"微分与梯度"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"公式和画图似乎不兼容md语法，数学公式会重复两遍，将就着看吧"}]},{"type":"element","tag":"h2","props":{"id":"微分"},"children":[{"type":"text","value":"微分"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"对于一元函数,描述了函数 $f$ 在 $x$ 处的变化率,直观表示为函数在该点的切线的斜率，为标量"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"$$ dy=f'(x)*dx $$"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"多元函数的全微分:\n$$df=\\frac{∂f}{∂x}*dx+\\frac{∂f}{∂y}*dy+\\frac{∂f}{∂z}*dz+...$$"}]},{"type":"element","tag":"h2","props":{"id":"梯度"},"children":[{"type":"text","value":"梯度"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"$$▽f(x,y)=gradf(x,y)=( \\frac{∂x}{∂f}, \\frac{∂y}{∂f})$$"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"梯度是一个向量，是多元函数在某一点处的偏导数构成的向量\n在一元函数中，导数与梯度在"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"数值"}]},{"type":"text","value":"上是相等的"}]},{"type":"element","tag":"h2","props":{"id":"自动微分求导autograd"},"children":[{"type":"text","value":"自动微分/求导(autograd)"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"pytorch中，设置了require_grads=True 的tensor可以被计算梯度。在backward(反向传播)过程中会被autograd模块自动计算梯度并保存在 tensor.grad"}]},{"type":"element","tag":"h2","props":{"id":"梯度下降"},"children":[{"type":"text","value":"梯度下降"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"沿着梯度向量的方向是函数增加的最快，更容易找到函数的极大值；\n反之，沿着梯度向量相反的地方，梯度减少的最快，更容易找到极小值。"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"梯度下降，即每一点都沿着梯度方向的反方向求解，找到极小值(不一定是最小值)，反之为梯度上升。"}]},{"type":"element","tag":"blockquote","props":{},"children":[{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"类比: 从山上下山，想要最快地到达山底，每一步都考虑最优，则每一步都沿着最陡峭的地方下山，但很可能走不到山脚，因为可能会走到山峰局部的低谷。"}]}]},{"type":"element","tag":"h1","props":{"id":"神经网络"},"children":[{"type":"text","value":"神经网络"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"markdown画的图有点丑，例如两个输入的神经网络\n在输入层(第一层)与输出层(最后一层)之间的都是隐藏层"}]},{"type":"element","tag":"pre","props":{"className":["language-mermaid"],"code":"graph LR;\n  subgraph 输入层\n  X((x1))\n  Y((x2))\n  end\n  subgraph 隐藏层\n  P((h1))\n  Q((h2))\n  end\n  subgraph 输出层  \n  F((o1))\n  end\n  X--w11-->P\n  Y--w21-->P\n  X--w12-->Q\n  Y--w22-->Q\n  P-->F\n  Q-->F\n","language":"mermaid","meta":""},"children":[{"type":"element","tag":"code","props":{"__ignoreMap":""},"children":[{"type":"text","value":"graph LR;\n  subgraph 输入层\n  X((x1))\n  Y((x2))\n  end\n  subgraph 隐藏层\n  P((h1))\n  Q((h2))\n  end\n  subgraph 输出层  \n  F((o1))\n  end\n  X--w11-->P\n  Y--w21-->P\n  X--w12-->Q\n  Y--w22-->Q\n  P-->F\n  Q-->F\n"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"图渲染不了，请自行搜索"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"神经网络简单示例"}]}]},{"type":"element","tag":"h2","props":{"id":"前向传播forward"},"children":[{"type":"text","value":"前向传播(forward)"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"将上一层的输出作为下一层的输入，并计算下一层的输出，一直到运算到输出层为止。\n$$h_1=w_{11} *x_1+w_{21}*x_2$$"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"$$h_2=w_{12} *x_1+w_{22}*x_2$$\n变为向量的形式:\n$$H=W *X+B$$\n当然无论多少次线性运算最终的结果都会与原来是线性相关的，所以会使用激活函数使每一层的计算结果变为非线性，作为下一层计算的输入。\n$$z^{(l)}=W^{(l)} *a^{(l-1)}+b^{(l)}$$\n$$a^{(l)}=σ(z)^{(l)}$$\n其中 $l$ 是层数"}]},{"type":"element","tag":"h2","props":{"id":"反向传播backforward"},"children":[{"type":"text","value":"反向传播(backforward)"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"数学知识比较复杂，已经好多年没学数学了~只简述其原理\n以下文章应该算是比较有价值的\n"},{"type":"element","tag":"a","props":{"href":"https://zhuanlan.zhihu.com/p/32368246","rel":["nofollow"]},"children":[{"type":"text","value":"https://zhuanlan.zhihu.com/p/32368246"}]},{"type":"element","tag":"a","props":{"href":"https://zhuanlan.zhihu.com/p/71892752","rel":["nofollow"]},"children":[{"type":"text","value":"https://zhuanlan.zhihu.com/p/71892752"}]}]},{"type":"element","tag":"h3","props":{"id":"链式法则"},"children":[{"type":"text","value":"链式法则"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"若$$y=g(x),z=f(g(x))=f(y),$$则"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"$$\\frac{dz}{dx}=\\frac{dz}{dy}*\\frac{dy}{dx}$$"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"神经网络显然是复杂的复合函数，在计算梯度时遵循链式法则"}]},{"type":"element","tag":"h3","props":{"id":"损失函数"},"children":[{"type":"text","value":"损失函数"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"度量训练样本计算出的输出和真实的训练样本输出之间的损失。我们使用最常见的均方误差（MSE）来作为损失函数，\n$$C(W,b)=\\frac{1}{2}||a^{(l)}-y||^2_2$$\n其中为训练样本计算出的输出，y为训练样本的真实值。加入系数 是为了抵消微分出来的指数。"}]},{"type":"element","tag":"h3","props":{"id":"反向传播算法"},"children":[{"type":"text","value":"反向传播算法"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"根据前向传播计算的输出与损失函数，根据链式法则推导，根据梯度下降更新神经网络，然后不断迭代得到最终的神经网络模型\n$$z^{(l)}=W^{(l)} *a^{(l-1)}+b^{(l)}$$\n$$a^{(l)}=σ(z)^{(l)}$$"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"感兴趣的可以看："},{"type":"element","tag":"a","props":{"href":"https://zhuanlan.zhihu.com/p/71892752","rel":["nofollow"]},"children":[{"type":"text","value":"https://zhuanlan.zhihu.com/p/71892752"}]}]},{"type":"element","tag":"h2","props":{"id":"总结"},"children":[{"type":"text","value":"总结"}]},{"type":"element","tag":"ol","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"初始化权重w与偏置b，"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"通过前向传播算法计算输出，"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"通过损失函数计算输出层的梯度"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"使用反向传播算法计算"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"通过梯度下降算法更新权重w与偏置b"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"当所有的w与b的变化值都小于迭代阈值时结束循环"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"输出各隐藏层与输出层的线性关系系数矩阵W和偏置b，得到确定的模型"}]}]}],"toc":{"title":"","searchDepth":2,"depth":3,"links":[{"id":"简单理解张量","depth":2,"text":"简单理解张量"},{"id":"常用-api","depth":2,"text":"常用 API"},{"id":"微分","depth":2,"text":"微分"},{"id":"梯度","depth":2,"text":"梯度"},{"id":"自动微分求导autograd","depth":2,"text":"自动微分/求导(autograd)"},{"id":"梯度下降","depth":2,"text":"梯度下降"},{"id":"前向传播forward","depth":2,"text":"前向传播(forward)"},{"id":"反向传播backforward","depth":2,"text":"反向传播(backforward)","children":[{"id":"链式法则","depth":3,"text":"链式法则"},{"id":"损失函数","depth":3,"text":"损失函数"},{"id":"反向传播算法","depth":3,"text":"反向传播算法"}]},{"id":"总结","depth":2,"text":"总结"}]}},"_type":"markdown","_id":"content:essay:25.pytorch-神经网络基础.md","_source":"content","_file":"essay/25.pytorch-神经网络基础.md","_stem":"essay/25.pytorch-神经网络基础","_extension":"md"}