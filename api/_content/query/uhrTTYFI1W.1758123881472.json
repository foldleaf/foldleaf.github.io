{"_path":"/essay/26.pytorch-transformer","_dir":"essay","_draft":false,"_partial":false,"_locale":"","title":"pytorch-transformer","description":"https://zhuanlan.zhihu.com/p/338817680https://blog.csdn.net/qq_38890412/article/details/120601834","subtitle":"pytorch transformer","index":26,"date":"2023-03-27","lunar_date":"闰二初六","year":"2023","month":"03","month_en":"Mar","day":"27","tag":"技术","tag_en":"TECH","cover":"/img/rabbit/026.jpg","categories":"python","mathjax":true,"body":{"type":"root","children":[{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"a","props":{"href":"https://zhuanlan.zhihu.com/p/338817680","rel":["nofollow"]},"children":[{"type":"text","value":"https://zhuanlan.zhihu.com/p/338817680"}]},{"type":"element","tag":"a","props":{"href":"https://blog.csdn.net/qq_38890412/article/details/120601834","rel":["nofollow"]},"children":[{"type":"text","value":"https://blog.csdn.net/qq_38890412/article/details/120601834"}]}]},{"type":"element","tag":"h1","props":{"id":"seq2seq从序列到序列"},"children":[{"type":"text","value":"Seq2Seq(从序列到序列)"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"将一个作为输入的序列映射为一个作为输出的序列，这一过程由编码（Encoder）输入与解码（Decoder）输出两个环节组成, 前者负责把序列编码成一个固定长度的向量，这个向量作为输入传给后者，输出可变长度的向量。\nRNN-CNN-Transformer都是如此"}]},{"type":"element","tag":"h1","props":{"id":"rnn循环神经网络"},"children":[{"type":"text","value":"RNN(循环神经网络)"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"简单来说，我们想翻译"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"I like deep learning"}]},{"type":"text","value":"为中文，将这七个汉字的输入序列映射为输出序列"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"我喜欢深度学习"}]},{"type":"text","value":"。在输出"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"我"}]},{"type":"text","value":"时，会根据输入"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"I"}]},{"type":"text","value":"来翻译，在输出"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"喜欢"}]},{"type":"text","value":"时，会根据"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"I like"}]},{"type":"text","value":"来翻译，输出"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"深度"}]},{"type":"text","value":"时会·根据"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"I like deep"}]},{"type":"text","value":"来翻译，输出"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"学习"}]},{"type":"text","value":"是，会根据"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"I like deep learning"}]},{"type":"text","value":"来翻译"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"RNN不能并行，指的是当前的状态会依赖前一个状态，前一个状态的计算结果会作为当前状态的输入。"}]},{"type":"element","tag":"h1","props":{"id":"cnn卷积神经网络"},"children":[{"type":"text","value":"CNN(卷积神经网络)"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"简单来说是窗口式遍历，要翻译"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"I like deep learning"}]},{"type":"text","value":"，假设每两个词一组，,"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"I like"}]},{"type":"text","value":","},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"like deep"}]},{"type":"text","value":","},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"deep learning"}]},{"type":"text","value":"，分别提取每组的特征进行计算，所以 CNN 能够并行。但是仅如此这个神经网络并不能覆盖整个句子，所以需要再叠加 CNN，将多组的特征再提取一次进行计算，"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"I like"}]},{"type":"text","value":","},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"like deep"}]},{"type":"text","value":"为一组以及"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"like deep"}]},{"type":"text","value":","},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"deep learning"}]},{"type":"text","value":"为一组，以此类推地叠加，最后得到结果。"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"CNN能并行，指的是每一组的计算都能够同时进行，并不会依赖其他组的计算结果。"}]},{"type":"element","tag":"h1","props":{"id":"transformer"},"children":[{"type":"text","value":"Transformer"}]},{"type":"element","tag":"h2","props":{"id":"self-attention自注意力机制"},"children":[{"type":"text","value":"self-attention(自注意力机制)"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"注意力机制: 权重分配机制，给需要关注的地方分配更高的权重，不相关的分配更低的权重。\n"},{"type":"element","tag":"a","props":{"href":"https://blog.csdn.net/qq_38890412/article/details/120601834","rel":["nofollow"]},"children":[{"type":"text","value":"https://blog.csdn.net/qq_38890412/article/details/120601834"}]}]},{"type":"element","tag":"h3","props":{"id":"xxt-的含义"},"children":[{"type":"text","value":"$XX^T$ 的含义"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"对于二维矩阵，用行向量$A_1,A_2$表示\n$$\nX=\n\\begin{bmatrix}\na_{11} & a_{12} \\\na_{21} & a_{22} \\\n\\end{bmatrix} =\n\\begin{bmatrix}\nA_1 \\\nA_2 \\\n\\end{bmatrix}\n$$"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"$$\nA_1=\n\\begin{bmatrix}\na_{11} & a_{12} \\\n\\end{bmatrix},\nA_2=\n\\begin{bmatrix}\na_{21} & a_{22} \\\n\\end{bmatrix}\n$$"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"$$\nXX^T=\n\\begin{bmatrix}\na_{11} & a_{12} \\\na_{21} & a_{22} \\\n\\end{bmatrix}\n·\n\\begin{bmatrix}\na_{11} & a_{21} \\\na_{12} & a_{22} \\\n\\end{bmatrix}=\n\\begin{bmatrix}\nA_1A_1 & A_1A_2 \\\nA_2A_1 & A_2A_2 \\\n\\end{bmatrix}\n$$\nN维矩阵同理，对于$XX^T$得到的矩阵，其元素为原矩阵$X$的n个向量的两两内积，向量的内积的几何意义是"}]},{"type":"element","tag":"ol","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"表征或计算两个向量之间的夹角"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"一个向量在另一个向量方向上的投影\n$$A_1·A_2=|A_1|*|A_2|*cos\\theta$$\n投影值大小和夹角则表示向量之间的相关性，投影值大则意味两个向量相关度高，夹角为90度则两者线性无关"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"故矩阵$ XX^T$是一个方阵，我们以行向量的角度理解，里面保存了每个向量和自己与其他向量进行内积运算的结果。"}]},{"type":"element","tag":"h3","props":{"id":"softmax函数的含义"},"children":[{"type":"text","value":"$Softmax$函数的含义"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"对 C 个元素中的第 i 个元素的softmax值为:\n$$\nSoftmax(z_i)=\\tfrac{e^{z_i}}{\\sum^C_e^{z_c}}\n$$\n如 {ln1,ln2,ln3} ,其softmax值分别为 1/6,2/6,3/6\n其意义在于将每个输出的结果都赋予一个概率值，比如在分类时可以用来表示属于每个类别的可能性"}]},{"type":"element","tag":"h3","props":{"id":"softmaxxxtx"},"children":[{"type":"text","value":"$Softmax(XX^T)X$"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"$XX^T$得到可以表示两两向量之间的相关性的值，Softmax函数将这个值转换成概率值，其实就是为了得到权重，$Softmax(XX^T)$再与原来的$X$的积，其实就意味着加权求和，得到向量之间的相关度。"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"将$X$作为一个句子，那么其中的向量则为一个个词向量，通过这个算法得到这个句子本身中字词之间的相关度，所以叫"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"自注意"}]},{"type":"text","value":"，例如对于"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"早上好"}]},{"type":"text","value":"这句话中"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"早"}]},{"type":"text","value":" "},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"上"}]},{"type":"text","value":" "},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"好"}]},{"type":"text","value":"三个字对于其中的"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"早"}]},{"type":"text","value":"来说，"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"早上"}]},{"type":"text","value":"的关联度就比"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"早早"}]},{"type":"text","value":"、"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"早好"}]},{"type":"text","value":"的关联度高。"}]},{"type":"element","tag":"h3","props":{"id":"q-k-v-矩阵"},"children":[{"type":"text","value":"Q K V 矩阵"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"深度学习中不会直接使用$X^T$,为了增强模型的拟合能力，会使用可训练的参数矩阵:\n$$W^Q,W^K,W^V$$\n输入矩阵$X$分别与它们相乘得到Q(query)、K(key)、V(value)"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"$Attention(Q,K,V)=softmax(\\tfrac{QK^T}{\\sqrt{d_k}})V$\n上述表示了Scaled Dot-Product Attention(缩放点积注意力)"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"${d_k}$为方差，总之就是使得Transformer在训练过程中的梯度值保持稳定"}]},{"type":"element","tag":"h2","props":{"id":"multi-headed-attention多头注意力机制"},"children":[{"type":"text","value":"multi-headed attention(多头注意力机制)"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"自注意力机制的缺陷就是：模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置， 使用多头注意力机制能增强self-attention关注多个位置的能力"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"例如"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"我养了一只猫，它很可爱"}]},{"type":"text","value":"中,"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"它"}]},{"type":"text","value":"可以指代前文的"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"猫"}]},{"type":"text","value":"，但从另一个维度(形容词的角度)看"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"可爱"}]},{"type":"text","value":"也是可以用来修饰"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"它"}]},{"type":"text","value":"的，"},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"它"}]},{"type":"text","value":"与两者都有一定关联，使用多头注意力会使原本输出的一个向量变为多个向量"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"① 输入句子\n② 将句子进行标记,转换成词向量矩阵 $X$\n③ 将 X 切分成 8 份，并与权重矩阵$W_i$ 相乘，构成输入向量$W_iX$\n④ 计算 Attention 权重矩阵：$$Attention(Q,K,V)=softmax(\\tfrac{QK^T}{\\sqrt{d_k}})V$$\n⑤ 最后将 8 个头的结果合并"}]},{"type":"element","tag":"h2","props":{"id":"位置编码"},"children":[{"type":"text","value":"位置编码"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"上面的计算只是知道词之间的关联关系，但并不知道可以将词放在句子的什么位置，即如何分析词序\n以前的位置编码:"}]},{"type":"element","tag":"ol","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"1-N分配:\n太阳晒屁股啦 "},{"type":"element","tag":"span","props":{},"children":[{"type":"text","value":"1 2 3 4 5"}]},{"type":"text","value":"\n问题:越后面的数字越大，体现不了权重"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"span","props":{},"children":[{"type":"text","value":"0,1"}]},{"type":"text","value":"分配:\n太阳晒屁股啦 "},{"type":"element","tag":"span","props":{},"children":[{"type":"text","value":"0 0.2 0.4 0.6 0.8 1"}]},{"type":"text","value":"\n问题:句子长度不同位置编码不一样"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Transformer模型的位置编码使用三角函数式的相对位置编码：有兴趣可以了解\n"},{"type":"element","tag":"a","props":{"href":"https://blog.csdn.net/u013853733/article/details/107853989","rel":["nofollow"]},"children":[{"type":"text","value":"https://blog.csdn.net/u013853733/article/details/107853989"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"a","props":{"href":"https://zhuanlan.zhihu.com/p/106644634","rel":["nofollow"]},"children":[{"type":"text","value":"https://zhuanlan.zhihu.com/p/106644634"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"a","props":{"href":"https://kazemnejad.com/blog/transformer_architecture_positional_encoding/","rel":["nofollow"]},"children":[{"type":"text","value":"https://kazemnejad.com/blog/transformer_architecture_positional_encoding/"}]}]}],"toc":{"title":"","searchDepth":2,"depth":3,"links":[{"id":"self-attention自注意力机制","depth":2,"text":"self-attention(自注意力机制)","children":[{"id":"xxt-的含义","depth":3,"text":"$XX^T$ 的含义"},{"id":"softmax函数的含义","depth":3,"text":"$Softmax$函数的含义"},{"id":"softmaxxxtx","depth":3,"text":"$Softmax(XX^T)X$"},{"id":"q-k-v-矩阵","depth":3,"text":"Q K V 矩阵"}]},{"id":"multi-headed-attention多头注意力机制","depth":2,"text":"multi-headed attention(多头注意力机制)"},{"id":"位置编码","depth":2,"text":"位置编码"}]}},"_type":"markdown","_id":"content:essay:26.pytorch-transformer.md","_source":"content","_file":"essay/26.pytorch-transformer.md","_stem":"essay/26.pytorch-transformer","_extension":"md"}